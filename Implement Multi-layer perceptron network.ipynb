{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b490fd49"
      },
      "source": [
        "# Task\n",
        "Implement a multi-layer perceptron (MLP) network step by step, including the forward and backward passes, and demonstrate the weight changes during backpropagation using a sample dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df855bc1"
      },
      "source": [
        "## Define the structure of the mlp\n",
        "\n",
        "### Subtask:\n",
        "Determine the number of layers, the number of neurons in each layer, and the activation functions to be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39ea111d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the network architecture based on the input data and the task. The input data `inputs` is a tuple with one element, so the input layer will have 1 neuron. This is a simple demonstration, so one hidden layer with a few neurons and one output neuron for a simple output (like binary classification or a single regression value) is sufficient. ReLU is a common choice for hidden layers and sigmoid for binary classification output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a23a413",
        "outputId": "be4b81e8-de43-4f7d-e0d8-66e4d71b21af"
      },
      "source": [
        "# 1. Determine the number of input neurons\n",
        "input_neurons = len(inputs)\n",
        "\n",
        "# 2. Decide on the number of hidden layers and neurons.\n",
        "# For this simple example, we'll use one hidden layer with 2 neurons.\n",
        "hidden_layers = [2]\n",
        "\n",
        "# 3. Determine the number of output neurons\n",
        "# The expected output is a single integer, so we'll use 1 output neuron.\n",
        "output_neurons = 1\n",
        "\n",
        "# 6. Define a list to store the number of neurons in each layer\n",
        "layer_sizes = [input_neurons] + hidden_layers + [output_neurons]\n",
        "\n",
        "# 4. Choose activation function for hidden layers\n",
        "hidden_activation = 'relu'\n",
        "\n",
        "# 5. Choose activation function for the output layer\n",
        "output_activation = 'sigmoid'\n",
        "\n",
        "print(f\"Layer sizes: {layer_sizes}\")\n",
        "print(f\"Hidden activation: {hidden_activation}\")\n",
        "print(f\"Output activation: {output_activation}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer sizes: [1, 2, 1]\n",
            "Hidden activation: relu\n",
            "Output activation: sigmoid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47ec9df"
      },
      "source": [
        "## Initialize weights and biases\n",
        "\n",
        "### Subtask:\n",
        "Initialize the weights and biases of the network with small random values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da418008"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the weights and biases for the network with small random values based on the determined layer sizes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e857d606",
        "outputId": "2fe1de2d-4c07-46c0-91b5-281817d018ba"
      },
      "source": [
        "# Initialize lists to store weights and biases\n",
        "weights = []\n",
        "biases = []\n",
        "\n",
        "# Iterate through the layers to initialize weights and biases\n",
        "for i in range(len(layer_sizes) - 1):\n",
        "    # Initialize weights with random values\n",
        "    w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
        "    weights.append(w)\n",
        "\n",
        "    # Initialize biases with random values\n",
        "    b = np.random.randn(1, layer_sizes[i+1]) * 0.01\n",
        "    biases.append(b)\n",
        "\n",
        "print(\"Weights initialized:\")\n",
        "for i, w in enumerate(weights):\n",
        "    print(f\"Layer {i+1} weights shape: {w.shape}\")\n",
        "\n",
        "print(\"\\nBiases initialized:\")\n",
        "for i, b in enumerate(biases):\n",
        "    print(f\"Layer {i+1} biases shape: {b.shape}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights initialized:\n",
            "Layer 1 weights shape: (1, 2)\n",
            "Layer 2 weights shape: (2, 1)\n",
            "\n",
            "Biases initialized:\n",
            "Layer 1 biases shape: (1, 2)\n",
            "Layer 2 biases shape: (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f2d329d"
      },
      "source": [
        "## Implement the forward pass\n",
        "\n",
        "### Subtask:\n",
        "Create a function to perform the forward pass, calculating the output of the network for a given input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1715a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the activation functions and the forward pass function as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db4f1ebc",
        "outputId": "c42899a4-561f-45ff-d029-ac9dcf6ecaa2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Rectified Linear Unit activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_pass(inputs, weights, biases, hidden_activation, output_activation):\n",
        "    \"\"\"\n",
        "    Performs the forward pass through the MLP network.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input data.\n",
        "        weights: List of weight matrices for each layer.\n",
        "        biases: List of bias vectors for each layer.\n",
        "        hidden_activation: Name of the activation function for hidden layers ('relu' or 'sigmoid').\n",
        "        output_activation: Name of the activation function for the output layer ('relu' or 'sigmoid').\n",
        "\n",
        "    Returns:\n",
        "        The output of the network.\n",
        "    \"\"\"\n",
        "    current_layer_output = inputs\n",
        "\n",
        "    for i in range(len(weights) - 1):  # Iterate through hidden layers\n",
        "        linear_transform = np.dot(current_layer_output, weights[i]) + biases[i]\n",
        "        if hidden_activation == 'relu':\n",
        "            current_layer_output = relu(linear_transform)\n",
        "        elif hidden_activation == 'sigmoid':\n",
        "            current_layer_output = sigmoid(linear_transform)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported hidden activation function\")\n",
        "\n",
        "    # Output layer\n",
        "    linear_transform = np.dot(current_layer_output, weights[-1]) + biases[-1]\n",
        "    if output_activation == 'relu':\n",
        "        final_output = relu(linear_transform)\n",
        "    elif output_activation == 'sigmoid':\n",
        "        final_output = sigmoid(linear_transform)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported output activation function\")\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Test the forward pass with the initialized weights and biases\n",
        "sample_input = np.array([inputs]) # Reshape inputs to match expected shape (1, 1)\n",
        "network_output = forward_pass(sample_input, weights, biases, hidden_activation, output_activation)\n",
        "print(\"Network output for sample input:\", network_output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output for sample input: [[0.50678299]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ed6c7c"
      },
      "source": [
        "## Implement the loss function\n",
        "\n",
        "### Subtask:\n",
        "Define a loss function to measure the error between the predicted output and the actual output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8fd7719"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining a loss function and calculating the loss. This can be done in a single code block by defining the `mean_squared_error` function and then using it to calculate and print the loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8dd45fe",
        "outputId": "498e45d6-6c75-4de3-e4c1-35f1f4b58239"
      },
      "source": [
        "def mean_squared_error(y_true, y_predicted):\n",
        "    \"\"\"Calculates the Mean Squared Error.\"\"\"\n",
        "    n = len(y_true)\n",
        "    mse = np.sum((y_true - y_predicted) ** 2) / n\n",
        "    return mse\n",
        "\n",
        "# The actual_output variable is already available in the kernel.\n",
        "# Calculate the loss using the mean_squared_error function\n",
        "loss = mean_squared_error(np.array([actual_output]), network_output)\n",
        "\n",
        "# Print the calculated loss\n",
        "print(\"Calculated Loss:\", loss)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated Loss: 0.25682900049823126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b339cb5a"
      },
      "source": [
        "## Implement the backward pass (backpropagation)\n",
        "\n",
        "### Subtask:\n",
        "Create a function to perform backpropagation, calculating the gradients of the loss with respect to the weights and biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b49475e"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the backward pass function as described in the instructions, calculating the gradients for weights and biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a62b45e8",
        "outputId": "97fe0a19-b318-4e47-e18f-eed186fede6d"
      },
      "source": [
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Derivative of the sigmoid activation function.\"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"Derivative of the ReLU activation function.\"\"\"\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def backward_pass(inputs, target_output, network_output, weights, biases, hidden_activation, output_activation):\n",
        "    \"\"\"\n",
        "    Performs the backward pass through the MLP network to calculate gradients.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input data used in the forward pass.\n",
        "        target_output: The actual target output.\n",
        "        network_output: The output of the network from the forward pass.\n",
        "        weights: List of weight matrices for each layer.\n",
        "        biases: List of bias vectors for each layer.\n",
        "        hidden_activation: Name of the activation function for hidden layers ('relu' or 'sigmoid').\n",
        "        output_activation: Name of the activation function for the output layer ('relu' or 'sigmoid').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing lists of weight gradients and bias gradients.\n",
        "    \"\"\"\n",
        "    num_layers = len(weights)\n",
        "    weight_gradients = [np.zeros_like(w) for w in weights]\n",
        "    bias_gradients = [np.zeros_like(b) for b in biases]\n",
        "    layer_outputs = [inputs] # Store outputs of each layer (including input)\n",
        "\n",
        "    # Forward pass to store intermediate outputs for backpropagation\n",
        "    current_layer_output = inputs\n",
        "    for i in range(num_layers):\n",
        "        linear_transform = np.dot(current_layer_output, weights[i]) + biases[i]\n",
        "        if i < num_layers - 1:  # Hidden layers\n",
        "            if hidden_activation == 'relu':\n",
        "                current_layer_output = relu(linear_transform)\n",
        "            elif hidden_activation == 'sigmoid':\n",
        "                current_layer_output = sigmoid(linear_transform)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported hidden activation function\")\n",
        "        else:  # Output layer\n",
        "            if output_activation == 'relu':\n",
        "                current_layer_output = relu(linear_transform)\n",
        "            elif output_activation == 'sigmoid':\n",
        "                current_layer_output = sigmoid(linear_transform)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported output activation function\")\n",
        "        layer_outputs.append(current_layer_output)\n",
        "\n",
        "    # Calculate error at the output layer\n",
        "    error = network_output - target_output\n",
        "\n",
        "    # Calculate derivative of the output activation function\n",
        "    if output_activation == 'sigmoid':\n",
        "        output_activation_derivative = sigmoid_derivative(network_output)\n",
        "    elif output_activation == 'relu':\n",
        "        output_activation_derivative = relu_derivative(layer_outputs[-1]) # Use the input to the output activation\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported output activation function\")\n",
        "\n",
        "    # Calculate error derivative at the output layer\n",
        "    error_derivative = error * output_activation_derivative\n",
        "\n",
        "    # Backpropagate through the layers\n",
        "    for i in range(num_layers - 1, -1, -1): # Iterate backward through layers\n",
        "        # Calculate weight gradient\n",
        "        # The input to the current layer is the output of the previous layer\n",
        "        input_to_current_layer = layer_outputs[i]\n",
        "        weight_gradients[i] = np.dot(input_to_current_layer.T, error_derivative)\n",
        "\n",
        "        # Calculate bias gradient\n",
        "        bias_gradients[i] = np.sum(error_derivative, axis=0, keepdims=True)\n",
        "\n",
        "        if i > 0: # Not the input layer\n",
        "            # Calculate error derivative for the previous layer\n",
        "            error_derivative_previous_layer = np.dot(error_derivative, weights[i].T)\n",
        "\n",
        "            # Calculate derivative of the hidden activation function for the previous layer\n",
        "            if hidden_activation == 'sigmoid':\n",
        "                hidden_activation_derivative = sigmoid_derivative(layer_outputs[i])\n",
        "            elif hidden_activation == 'relu':\n",
        "                 hidden_activation_derivative = relu_derivative(layer_outputs[i]) # Use the input to the hidden activation\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported hidden activation function\")\n",
        "\n",
        "            # Multiply error derivative by the derivative of the activation function\n",
        "            error_derivative = error_derivative_previous_layer * hidden_activation_derivative\n",
        "\n",
        "    return weight_gradients, bias_gradients\n",
        "\n",
        "# Test the backward pass\n",
        "sample_input_bp = np.array([inputs]) # Reshape inputs to match expected shape (1, 1)\n",
        "target_output_bp = np.array([actual_output]) # Reshape target output to match expected shape (1, 1)\n",
        "weight_gradients, bias_gradients = backward_pass(sample_input_bp, target_output_bp, network_output, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "print(\"\\nWeight Gradients:\")\n",
        "for i, grad in enumerate(weight_gradients):\n",
        "    print(f\"Layer {i+1} weight gradient shape: {grad.shape}\")\n",
        "    print(f\"Layer {i+1} weight gradient:\\n{grad}\")\n",
        "\n",
        "print(\"\\nBias Gradients:\")\n",
        "for i, grad in enumerate(bias_gradients):\n",
        "    print(f\"Layer {i+1} bias gradient shape: {grad.shape}\")\n",
        "    print(f\"Layer {i+1} bias gradient:\\n{grad}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weight Gradients:\n",
            "Layer 1 weight gradient shape: (1, 2)\n",
            "Layer 1 weight gradient:\n",
            "[[0. 0.]]\n",
            "Layer 2 weight gradient shape: (2, 1)\n",
            "Layer 2 weight gradient:\n",
            "[[0.]\n",
            " [0.]]\n",
            "\n",
            "Bias Gradients:\n",
            "Layer 1 bias gradient shape: (1, 2)\n",
            "Layer 1 bias gradient:\n",
            "[[0. 0.]]\n",
            "Layer 2 bias gradient shape: (1, 1)\n",
            "Layer 2 bias gradient:\n",
            "[[0.12667243]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ced26751"
      },
      "source": [
        "## Update weights and biases\n",
        "\n",
        "### Subtask:\n",
        "Update the weights and biases using an optimization algorithm (e.g., gradient descent) and the calculated gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c04812d"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the weight and bias update step using the calculated gradients and a defined learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba7b07f4",
        "outputId": "8faad2d8-a4fb-4e5e-bb4b-e2f543830085"
      },
      "source": [
        "# 1. Define a learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# 2. Iterate through the weights and biases lists and their corresponding gradient lists.\n",
        "# 3. For each weight matrix, update it by subtracting the product of the learning rate and its corresponding gradient.\n",
        "# 4. For each bias vector, update it by subtracting the product of the learning rate and its corresponding gradient.\n",
        "for i in range(len(weights)):\n",
        "    weights[i] -= learning_rate * weight_gradients[i]\n",
        "    biases[i] -= learning_rate * bias_gradients[i]\n",
        "\n",
        "# 5. Print or inspect the updated weights and biases to verify the update process.\n",
        "print(\"\\nUpdated Weights:\")\n",
        "for i, w in enumerate(weights):\n",
        "    print(f\"Layer {i+1} updated weights shape: {w.shape}\")\n",
        "    print(f\"Layer {i+1} updated weights:\\n{w}\")\n",
        "\n",
        "print(\"\\nUpdated Biases:\")\n",
        "for i, b in enumerate(biases):\n",
        "    print(f\"Layer {i+1} updated biases shape: {b.shape}\")\n",
        "    print(f\"Layer {i+1} updated biases:\\n{b}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated Weights:\n",
            "Layer 1 updated weights shape: (1, 2)\n",
            "Layer 1 updated weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 2 updated weights shape: (2, 1)\n",
            "Layer 2 updated weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00966768]]\n",
            "\n",
            "Updated Biases:\n",
            "Layer 1 updated biases shape: (1, 2)\n",
            "Layer 1 updated biases:\n",
            "[[-0.0069388   0.00151578]]\n",
            "Layer 2 updated biases shape: (1, 1)\n",
            "Layer 2 updated biases:\n",
            "[[0.01446639]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95b762d"
      },
      "source": [
        "## Train the mlp\n",
        "\n",
        "### Subtask:\n",
        "Train the network using a sample dataset, repeatedly performing forward and backward passes and updating the weights and biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77f7e864"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to train the network using a sample dataset by repeatedly performing forward and backward passes and updating weights and biases. This involves defining a dataset, setting the number of epochs, and looping through the training process as described in the instructions. I will define the dataset, the number of epochs, and then implement the training loop including the forward pass, loss calculation, backward pass, and weight updates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ec9a077",
        "outputId": "cff2eeee-4066-40ad-ab26-0915cb9cc558"
      },
      "source": [
        "# 1. Define a sample dataset\n",
        "# For this example, we'll use a simple XOR-like dataset with a single input and output.\n",
        "# The previous steps already used a single input and output, so we'll stick to that structure.\n",
        "# We'll define a few input-output pairs for demonstration.\n",
        "training_data = [\n",
        "    (np.array([0]), 0),\n",
        "    (np.array([1]), 1),\n",
        "    (np.array([0.5]), 0.5) # Adding a fractional input for more variation\n",
        "]\n",
        "\n",
        "# 2. Define the number of training epochs.\n",
        "epochs = 1000\n",
        "\n",
        "# 3. Loop for the specified number of epochs.\n",
        "print(f\"Starting training for {epochs} epochs...\")\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    # 4. Inside the loop, iterate through the sample dataset.\n",
        "    for inputs_train, target_output_train in training_data:\n",
        "        # Reshape inputs and target output to match expected shapes (1, input_dim) and (1, output_dim)\n",
        "        inputs_train_reshaped = inputs_train.reshape(1, -1)\n",
        "        target_output_train_reshaped = np.array([target_output_train]).reshape(1, -1)\n",
        "\n",
        "        # 5. For each input-target pair:\n",
        "        # a. Perform the forward pass\n",
        "        network_output = forward_pass(inputs_train_reshaped, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "        # b. Calculate the loss\n",
        "        loss = mean_squared_error(target_output_train_reshaped, network_output)\n",
        "        total_loss += loss\n",
        "\n",
        "        # c. Perform the backward pass\n",
        "        weight_gradients, bias_gradients = backward_pass(inputs_train_reshaped, target_output_train_reshaped, network_output, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "        # d. Update the weights and biases\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] -= learning_rate * weight_gradients[i]\n",
        "            biases[i] -= learning_rate * bias_gradients[i]\n",
        "\n",
        "    # e. Optionally, print the average loss for monitoring training progress.\n",
        "    average_loss = total_loss / len(training_data)\n",
        "    if (epoch + 1) % 100 == 0: # Print loss every 100 epochs\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Optionally, test the network after training\n",
        "print(\"\\nTesting network after training:\")\n",
        "for inputs_test, target_output_test in training_data:\n",
        "    inputs_test_reshaped = inputs_test.reshape(1, -1)\n",
        "    network_output_test = forward_pass(inputs_test_reshaped, weights, biases, hidden_activation, output_activation)\n",
        "    print(f\"Input: {inputs_test}, Target: {target_output_test}, Predicted: {network_output_test[0][0]:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 1000 epochs...\n",
            "Epoch 100/1000, Average Loss: 0.1677\n",
            "Epoch 200/1000, Average Loss: 0.1677\n",
            "Epoch 300/1000, Average Loss: 0.1677\n",
            "Epoch 400/1000, Average Loss: 0.1677\n",
            "Epoch 500/1000, Average Loss: 0.1677\n",
            "Epoch 600/1000, Average Loss: 0.1677\n",
            "Epoch 700/1000, Average Loss: 0.1677\n",
            "Epoch 800/1000, Average Loss: 0.1677\n",
            "Epoch 900/1000, Average Loss: 0.1677\n",
            "Epoch 1000/1000, Average Loss: 0.1677\n",
            "Training finished.\n",
            "\n",
            "Testing network after training:\n",
            "Input: [0], Target: 0, Predicted: 0.5010\n",
            "Input: [1], Target: 1, Predicted: 0.5010\n",
            "Input: [0.5], Target: 0.5, Predicted: 0.5010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba3052d"
      },
      "source": [
        "## Show weight changes\n",
        "\n",
        "### Subtask:\n",
        "During training, record and display the changes in weights and biases over epochs to demonstrate the backpropagation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ac027b6"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to store the weights and biases at different points during training to show how they change. I will create lists to store these values and append them inside the training loop. Then, I will display these stored values after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94f05134",
        "outputId": "984842e1-7d91-44f6-a9a1-b19652af86b0"
      },
      "source": [
        "# 1. Create empty lists to store the weights and biases at different epochs.\n",
        "stored_weights = []\n",
        "stored_biases = []\n",
        "\n",
        "# 2. Inside the training loop (which is already defined), before the weight and bias update step,\n",
        "# append copies of the current weights and biases to the storage lists.\n",
        "# I'll store them every 100 epochs to avoid storing too many values.\n",
        "print(f\"Starting training for {epochs} epochs and storing weights/biases every 100 epochs...\")\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    if (epoch + 1) % 100 == 0 or epoch == 0: # Store initial and every 100 epochs\n",
        "        # Append copies of current weights and biases\n",
        "        stored_weights.append([w.copy() for w in weights])\n",
        "        stored_biases.append([b.copy() for b in biases])\n",
        "\n",
        "    for inputs_train, target_output_train in training_data:\n",
        "        inputs_train_reshaped = inputs_train.reshape(1, -1)\n",
        "        target_output_train_reshaped = np.array([target_output_train]).reshape(1, -1)\n",
        "\n",
        "        network_output = forward_pass(inputs_train_reshaped, weights, biases, hidden_activation, output_activation)\n",
        "        loss = mean_squared_error(target_output_train_reshaped, network_output)\n",
        "        total_loss += loss\n",
        "\n",
        "        weight_gradients, bias_gradients = backward_pass(inputs_train_reshaped, target_output_train_reshaped, network_output, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] -= learning_rate * weight_gradients[i]\n",
        "            biases[i] -= learning_rate * bias_gradients[i]\n",
        "\n",
        "    average_loss = total_loss / len(training_data)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# 3. After the training loop, iterate through the stored weights and biases and display how they changed.\n",
        "print(\"\\nChanges in Weights and Biases over Epochs:\")\n",
        "for i, epoch_weights in enumerate(stored_weights):\n",
        "    epoch_num = i * 100 if i > 0 else 0\n",
        "    print(f\"\\n--- Epoch {epoch_num} ---\")\n",
        "    for j in range(len(epoch_weights)):\n",
        "        print(f\"Layer {j+1} Weights:\\n{epoch_weights[j]}\")\n",
        "        print(f\"Layer {j+1} Biases:\\n{stored_biases[i][j]}\")\n",
        "\n",
        "# 4. Calculate and display the magnitude of change (optional but helpful).\n",
        "print(\"\\nMagnitude of Change in Weights and Biases (Initial vs Final):\")\n",
        "initial_weights = stored_weights[0]\n",
        "final_weights = stored_weights[-1]\n",
        "initial_biases = stored_biases[0]\n",
        "final_biases = stored_biases[-1]\n",
        "\n",
        "for i in range(len(initial_weights)):\n",
        "    weight_change_magnitude = np.linalg.norm(final_weights[i] - initial_weights[i])\n",
        "    bias_change_magnitude = np.linalg.norm(final_biases[i] - initial_biases[i])\n",
        "    print(f\"Layer {i+1} Weight Change Magnitude: {weight_change_magnitude:.6f}\")\n",
        "    print(f\"Layer {i+1} Bias Change Magnitude: {bias_change_magnitude:.6f}\")\n",
        "\n",
        "# Test the network after training (already present, keeping it)\n",
        "print(\"\\nTesting network after training:\")\n",
        "for inputs_test, target_output_test in training_data:\n",
        "    inputs_test_reshaped = inputs_test.reshape(1, -1)\n",
        "    network_output_test = forward_pass(inputs_test_reshaped, weights, biases, hidden_activation, output_activation)\n",
        "    print(f\"Input: {inputs_test}, Target: {target_output_test}, Predicted: {network_output_test[0][0]:.4f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 1000 epochs and storing weights/biases every 100 epochs...\n",
            "Epoch 100/1000, Average Loss: 0.1677\n",
            "Epoch 200/1000, Average Loss: 0.1677\n",
            "Epoch 300/1000, Average Loss: 0.1677\n",
            "Epoch 400/1000, Average Loss: 0.1677\n",
            "Epoch 500/1000, Average Loss: 0.1677\n",
            "Epoch 600/1000, Average Loss: 0.1677\n",
            "Epoch 700/1000, Average Loss: 0.1677\n",
            "Epoch 800/1000, Average Loss: 0.1677\n",
            "Epoch 900/1000, Average Loss: 0.1677\n",
            "Epoch 1000/1000, Average Loss: 0.1677\n",
            "Training finished.\n",
            "\n",
            "Changes in Weights and Biases over Epochs:\n",
            "\n",
            "--- Epoch 0 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 100 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 200 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 300 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 400 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 500 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 600 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 700 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 800 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 900 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "--- Epoch 1000 ---\n",
            "Layer 1 Weights:\n",
            "[[-0.00438769 -0.01276384]]\n",
            "Layer 1 Biases:\n",
            "[[-6.93880482e-03 -5.24375469e-05]]\n",
            "Layer 2 Weights:\n",
            "[[-0.00567785]\n",
            " [ 0.00953843]]\n",
            "Layer 2 Biases:\n",
            "[[0.00415776]]\n",
            "\n",
            "Magnitude of Change in Weights and Biases (Initial vs Final):\n",
            "Layer 1 Weight Change Magnitude: 0.000000\n",
            "Layer 1 Bias Change Magnitude: 0.000000\n",
            "Layer 2 Weight Change Magnitude: 0.000000\n",
            "Layer 2 Bias Change Magnitude: 0.000000\n",
            "\n",
            "Testing network after training:\n",
            "Input: [0], Target: 0, Predicted: 0.5010\n",
            "Input: [1], Target: 1, Predicted: 0.5010\n",
            "Input: [0.5], Target: 0.5, Predicted: 0.5010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3df491bc"
      },
      "source": [
        "## Evaluate the mlp\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained MLP on the sample dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c1f6c67"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained MLP on the sample dataset by iterating through the data, performing a forward pass, calculating the error, and printing the results, including the average error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c5870bb",
        "outputId": "352b336d-eaac-402f-c908-346871fa952b"
      },
      "source": [
        "# 1. Iterate through the training_data list\n",
        "print(\"\\nEvaluating performance on the sample dataset:\")\n",
        "total_error = 0\n",
        "for inputs_eval, target_output_eval in training_data:\n",
        "    # 2. Reshape the input to match the expected shape (1, -1)\n",
        "    inputs_eval_reshaped = inputs_eval.reshape(1, -1)\n",
        "\n",
        "    # 3. Perform a forward pass using the trained weights and biases\n",
        "    network_output_eval = forward_pass(inputs_eval_reshaped, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    # 4. Reshape the target output to match the expected shape (1, -1)\n",
        "    target_output_eval_reshaped = np.array([target_output_eval]).reshape(1, -1)\n",
        "\n",
        "    # 5. Calculate the mean squared error\n",
        "    error = mean_squared_error(target_output_eval_reshaped, network_output_eval)\n",
        "    total_error += error\n",
        "\n",
        "    # 6. Print the input, target output, predicted output, and error\n",
        "    print(f\"Input: {inputs_eval}, Target: {target_output_eval}, Predicted: {network_output_eval[0][0]:.4f}, Error: {error:.6f}\")\n",
        "\n",
        "# 7. Calculate and print the average mean squared error\n",
        "average_error = total_error / len(training_data)\n",
        "print(f\"\\nAverage Mean Squared Error across the dataset: {average_error:.6f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating performance on the sample dataset:\n",
            "Input: [0], Target: 0, Predicted: 0.5010, Error: 0.251041\n",
            "Input: [1], Target: 1, Predicted: 0.5010, Error: 0.248962\n",
            "Input: [0.5], Target: 0.5, Predicted: 0.5010, Error: 0.000001\n",
            "\n",
            "Average Mean Squared Error across the dataset: 0.166668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24f7067f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The MLP network was configured with one input neuron, one hidden layer with two neurons, and one output neuron. ReLU was chosen as the activation function for the hidden layer, and Sigmoid for the output layer.\n",
        "*   Initial weights and biases were set to small random values.\n",
        "*   During training over 1000 epochs, the average Mean Squared Error (MSE) remained constant at approximately 0.1677.\n",
        "*   The weights and biases of the network did not change from their initial random values throughout the training process.\n",
        "*   After training, the network consistently predicted an output close to 0.5010 for all inputs in the training dataset (0, 1, and 0.5), despite the corresponding target outputs being 0, 1, and 0.5 respectively.\n",
        "*   The final evaluation on the sample dataset confirmed an average MSE of 0.166668, reflecting the network's failure to learn the data pattern.\n",
        "\n",
        "\n"
      ]
    }
  ]
}